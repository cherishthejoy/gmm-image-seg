---
jupyter: python3
title: "Gaussian Mixture Model Image Segmentation"
subtitle: ""
abstract: Энэхүү ажлаар өөр хэмжээст бүхий санамсаргүй утгууд дээр хэвийн тархалтыг ашиглан граафик гаргах, тэр дундаас гурван хэмжээст утга бүхий зурган өгөгдөл дээр Гауссын хольцийн моделийг ашиглан сэгмэнтлэн, хангалтгүй үр дүн үзүүлэх тохиолдлуудыг тодорхойлон дүгнэх болно. Мөн тухайн моделийн параметрүүд ямар утгуудад ямар тохиолдолд үр дүн өгөөжтэй байхыг тодорохойлох.
author: "Team"
date: "2025-11-06"
date-format: "YYYY оны M-р сарын D"
toc: true
toc-depth: 3
toc-title: Агуулга
number-sections: true
format:
  pdf:
    fig-pos: "H"
    echo: true
    pdf-engine: xelatex
    papersize: a4paper
    geometry:
      - left=2cm
      - right=2cm
      - top=2cm
      - bottom=3cm
    include-in-header:
      - text: |
          \usepackage[english,mongolian]{babel}
          \usepackage{fontspec}
          % үндсэн текстийн шрифт
          \setmainfont{Times New Roman}
          % код хэсгийн шрифт
          \setmonofont{DejaVu Sans Mono}
          \AddToHook{env/Highlighting/begin}{\footnotesize}
          % үндсэн гарчиг
          \usepackage{titling}
          \pretitle{\begin{center}\LARGE\bfseries}
          \posttitle{\par\end{center}\vskip 1em}
          % сэдвийн зүйлчлэл хэсгийн шрифт
          \usepackage{titlesec}
          \titleformat{\section}{\normalfont\Large\bfseries\selectfont}{\thesection}{1em}{}
          \titleformat{\subsection}{\normalfont\large\bfseries\selectfont}{\thesubsection}{1em}{}
          \titleformat{\subsubsection}{\normalfont\normalsize\bfseries\selectfont}{\thesubsubsection}{1em}{}
          % сэдвийн жагсаалт доторх шрифт
          \usepackage{tocloft}
          \renewcommand{\cfttoctitlefont}{\Large\bfseries\fontspec{Times New Roman}}
          \renewcommand{\cftaftertoctitle}{\vskip 1em}
          \renewcommand{\cftsecfont}{\normalfont\selectfont}
          \renewcommand{\cftsecpagefont}{\normalfont\selectfont}
          \renewcommand{\cftsubsecfont}{\normalfont\selectfont}
          \renewcommand{\cftsubsecpagefont}{\normalfont\selectfont}
          \renewcommand{\contentsname}{Агуулга}
    latex-max-runs: 3
editor: source
execute:
  echo: false
crossref: 
  fig-title: Зураг
  fig-prefix: Зураг
  tbl-title: Хүснэгт
  tbl-prefix: Хүснэгт
fig-align: center
fig-env: "figure"
fig-height: 4
fig-width: 6
fig-pos: "!ht"
fig-format: pdf
fig-cap-location: top
tbl-cap-location: top
bibliography: references.bib
csl: ieee.csl
citeproc: true
link-citations: true
---

# Шаардлагатай багцууд {-}

Шаардлагатай багцуудыг дараах байдлаар урьдчилан суулгана.

`pip install numpy scikit-learn Pillow matplotlib`

# Өгөгдөл

Голлон ашиглах өгөгдөл нь зурган өгөгдөл байх бөгөөд "jpg", "jpeg", "png" гэсэн өргөтгэлтэй байна. Эндээс "png" өргөтгөлтэй зургууд нь ихэвчлэн альфа сувгын мэдээллийг агуулдаг учир зохих аргыг ашиглана мэдээллийг зохих хэлбэрт оруулна. Зургуудыг хангалттай болон хангалтгүй үр дүнд хүрэх гэж 2 ангилах бөгөөд, GMM ашиглан хангалттай буюу segmentation хийхэд тохиромжтой, зураг дээрх обьектууд олон өнгөөс тогтсон, гэрлийн чанар муу гэх тодотголууд дээр суурилан хангалтгүй гэж тус тусад нь тодорхойлж болно. Нэг болон хоёр хэмжээст хэвийн тархалтуудын хувьд scikit-learn сангын синтетик өгөгдөл үүсгэх аргыг ашиглах болно. Мөн энэ тайланд Гауссын хольцийн моделийг "ГХМ" гэж товчилсон болно.

# Нэг хэмжээст ГХМ

```{python}
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from sklearn.mixture import GaussianMixture as GMM
from matplotlib.colors import LogNorm
import matplotlib.ticker as ticker
```

Синтетик аргаар хоёр өөр төрлийн хэвийн тархалт бүхий өгөгдлүүдийг үүсгээд, тооцоолол хийх боломжтой хэлбэрт оруулна.

$$
\mu_1, \mu_2 = \text{Дундаж}
$$
$$
\sigma^2_1, \sigma^2_2  = \text{Стандарт хазайлт}
$$
$$
\pi = \text{Хольцийн жин}
$$  

Синтетик өгөгдөл үүсгэхдээ дээрх параметрүүдийг тодорхойлон тархалтуудыг үүсгэнэ.
```{python}
rng = np.random.default_rng()
x = np.concatenate((rng.normal(1, 2, 1000), rng.normal(9, 3, 1000)))
f = x.reshape(-1, 1)
```

Гауссын хольцийн моделд зохих ковариансын матриксийн төрөл болон кластерийн тоог өгнө. Ковариансын матрицийн төрлүүдийг дараа дахин авч үзнэ.

```{python}
gmm = GMM(n_components=2, covariance_type='full')
gmm.fit(f)
x_axis = np.sort(x)
```

Постериор магадлалын томьёонд орлуулга хийн цэгэн утга бүрийн магадлалыг олно. Гамма нь тухайн ажиглагдсан цэгэн өгөдөл нь аль кластэрт хамаарах вэ гэдэг асуултад тодорхой хариулт өгдөг. Гамма болон latent-variable -ийн талаар төгсгөл хэсэгт дурдах болно.
$$
\gamma_i = \frac{\pi_1 \cdot \phi_2(x_i|\mu_2,\sigma^2_2)}{\pi_2 \cdot \phi_1(x_i|\mu_1,\sigma^2_1) + \pi_1 \cdot \phi_2(x_i|\mu_2,\sigma^2_2)}
$$

Бидний өгөгдөл 2 хэвийн тархалттай кластер учир:

$$
\gamma_j = 1 - \gamma_i
$$байна.

$$
\pi = \sum_{i = 1}^N(1-\gamma_i)/N
$$
$$
\mu_1 = \frac{\sum_{i = 1}^N(1-\gamma_i)x_i}{\sum_{i = 1}^N(1-\gamma_i)}
$$
$$
\mu_2 = \frac{\sum_{i = 1}^N\gamma_ix_i}{\sum_{i = 1}^N\gamma_i}
$$
$$
\sigma_1 =
\sqrt{
\frac{\sum_{i = 1}^N(1-\gamma_i)(x_i - \mu_1)^2}{\sum_{i = 1}^N(1-\gamma_i)}
}
$$
$$
\sigma_2 =
\sqrt{
\frac{\sum_{i = 1}^N\gamma_i(x_i - \mu_2)^2}{\sum_{i = 1}^N\gamma_i}
}$$

Сургасан моделоор тооцоолсон $\mu_1, \mu_2, \sigma_1, \sigma_2, \pi$ параметрүүдийн боломжит хамгийн их утгуудыг тооцоолон нягтын функцийн графикийг дүрслэнэ.

```{python}
#| label: fig-1d-pdf
#| fig-cap: Синтетик өгөгдөлтэй 2 өөр хэвийн тархалтын нягтын функц

weights = gmm.weights_
means = gmm.means_
covars = gmm.covariances_

plt.hist(f, bins=100, histtype='bar', density=True, ec='white', alpha=1.0)
plt.plot(x_axis, weights[0] * stats.norm.pdf(x_axis, means[0], np.sqrt(covars[0])).ravel(), c = 'red')
plt.plot(x_axis, weights[1] * stats.norm.pdf(x_axis, means[1], np.sqrt(covars[1])).ravel(), c = 'green')

plt.grid()
plt.show()
```

\pagebreak

# Хоёр хэмжээст ГХМ

Хоёр хэмжээст хэвийн тархалтын санамсаргүй утгууд нь вектор байх ба өгөгдлийн хэмжээсийг дагаж параметрүүдийн хэмжээс ихэснэ.

Өгөдлийг өмнөхтэй адил синтетикээр үүсгэх бөгөөд стандарт хазайлт нь коварианс матриц хэлбэртэй болно. Коварианс матриц нь эллипсийн хоёр хэмжээст огторгуйн тэнхлэг болон хэмжээст шууд нөлөөлнө.

Доорх код нь гурван өөр төрлийн эллипс хэмжээстэй энгийн тархалт бүхий өгөгдлийг үүсгэнэ.

```{python}

n_samples = 300
rng = np.random.default_rng()
c1 = rng.standard_normal((n_samples, 2)) + np.array([20, 20])
dot_two = np.array([[0.0, -0.7], [3.5, 0.7]])
c2 = np.dot(rng.standard_normal((n_samples, 2)), dot_two)
dot_three = np.array([[0.0, 0.5], [0.5, 0.7]])
c3 = np.dot(rng.standard_normal((n_samples, 2)), dot_three) + np.array([-10, 30])
X_train = np.vstack([c1, c2, c3])

gmm = GMM(n_components=3, covariance_type="full").fit(X_train)
```
Гурван өөр төрлийн хэвийн тархалттай учир, кластерийн тоог '3' гэж өгнө.

ГХМ нь голлон EM алгоритмийг ашиглан моделийг сургадаг. Алгоритмийн тодорхой үе шатад likelihood утгуудын үржвэр нь өмнөх үржвэрийн утгаас төдийлөн ялагарахгүй бол параметрийн үнэлэлт нь эцэсдээ хүрсэн гэж тооцдог.

$$
\mathcal{L}(\theta|x_i) = \prod_{i=1}^N f(x_i|\theta)
$$

Likelihood-ийг тооцоолоход цэгэн утга болгоныг үржүүлэх шаардлага гардаг учир логарифм авснаар зөвхөн нийлбэрийг тооцолохоор болгон хөнгөвчилдөг. Энэ нөхцөлд параметр үнэлэлт нь логарифм нийлбэр өөрчлөлт гарахгүй болох хүртэл явагдана гэсэн үг. Ийнхүү логарифм likelihood (үнэний ор) -ийг олох нь олон давуу талтай гэж тайлбарлаж болно.

$$
\ell(\theta|x_i) = \log\left(\prod_{i=1}^N f(x_i|\theta)\right) = \sum_{i=1}^N \log f(x_i|\theta)
$$
\pagebreak

Хоёр хэмжээст огторгуйд нягтын функцийг дүрслэх 2 хэмжээст огторгуйг үүсгэнэ.
```{python}
x = np.linspace(-40.0, 50.0, 200)
y = np.linspace(-40.0, 50.0, 200)
X, Y = np.meshgrid(x, y)
XX = np.array([X.ravel(), Y.ravel()]).T
Z = -gmm.score_samples(XX)
Z = Z.reshape(X.shape)
``` 


Хоёр хэмжээст огторгуйд нягтын функцийн шугаман загварыг дүрслэнэ.
```{python}
#| label: fig-negative-log
#| fig-cap: Хоёр хэмжээст хэвийн тархалтын сөрөг log-likelihood
fmt = ticker.LogFormatterMathtext()
fmt.create_dummy_axis()

CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), 
                levels=np.logspace(0, 3, 10))

plt.clabel(CS, fmt=fmt, fontsize=5)

plt.scatter(X_train[:, 0], X_train[:, 1], 0.8, color='white', marker="+")
plt.gca().set_aspect('equal')
plt.show()
``` 
\pagebreak

# Гурван хэмжээст ГХМ болон Image Segmentation

Өмнө нь нэг болон хоёр хэмжээст ГХМ моделийг синтетик өгөгдөл дээр суурилан сургах талаар авч үзсэн. Нэг хэмжээст өгөгдөлд гэрлийн эрчим (intensity), хоёр хэмжээст өгөгдөлд [x, y] тэнхлэгт байх цэгэн утга тохиромжтой бол гурван хэмжээст өгөгдлийн хувьд зурган өгөгдлийн RGB суваг нь тохиромжтой гэж төсөөлж болно. 

$$
\begin{bmatrix}
X_1
\cr
X_2
\cr
\vdots
\cr
X_n
\end{bmatrix}
\sim
\mathcal{N}
\left(
\begin{bmatrix}
\mu_{X_1}
\cr
\mu_{X_2}
\cr
\vdots
\cr
\mu_{X_n}
\end{bmatrix},
\begin{bmatrix}
\sigma_{X_1}^2 & \sigma_{X_1, X_2} & \dots & \sigma_{X_1, X_n}
\cr
\sigma_{X_2, X_1} & \sigma_{X_2}^2 & & \vdots
\cr
\vdots & & \ddots
\cr
\sigma_{X_n, X_1} & \ldots & & \sigma_{X_n}^2
\end{bmatrix}
\right)
$$


$$
\mu_i = {\mu_{R}, \mu_{G}, \mu_{B}}
$$

$$
\Sigma_i = 
\begin{bmatrix}
\sigma^2_{R} & \sigma_{RG} & \sigma_{RB}
\cr
\sigma_{RG} & \sigma^2_{G} & \sigma_{GB}
\cr
\sigma_{RB} & \sigma_{GB} & \sigma^2_{B}
\end{bmatrix}
$$

```{python}
import numpy as np
from PIL import Image
from IPython.display import display
import warnings
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt

from sklearn.mixture import GaussianMixture as GM
```

Ашиглагдах зурган өгөгдлийг оруулах.
```{python}
img = Image.open("images/fruits.jpg")
```

Зурган өгөгдөл нь альфа (A) сувгийн мэдээллийг агуулдаг бол энгийн 'RGB' форматад оруулан хадгална.
```{python}
img = img.convert('RGB')
img2 = np.array(img)
rows, cols, ch = img2.shape
img3 = img2.reshape((-1, 3))
```

Өгөгдлөөс зөвхөн 1000 цэгийг санамсаргүйгээр сонгон авч ковариацийн матрицийн 4 төрөлд тус тусад нь сургана.
```{python}
results = []
cov_types = ['full', 'tied', 'spherical', 'diag']
idx = np.random.choice(len(img3), size = 1000, replace=False)

for cov_type in cov_types:
    gmm = GM(n_components=3, covariance_type=cov_type).fit(img3)
    labels = gmm.predict(img3)

    sample = img3[idx]
    sample_labels = labels[idx]
    results.append((cov_type, sample, sample_labels))
```

Цэгэн өгөгдлүүдийг кластерт хамаарах байдлаар өөр өнгүүдээр дүрслэх.
```{python}
#| label: fig-random-data-points
#| fig-cap: Зурган өгөгдлөөс санаммаргүйгээр сонгосон цэгүүдийн тархалтын график
plt.style.use('default')
fig, axes = plt.subplots(2, 2, figsize=(12, 12), subplot_kw={'projection': '3d'})
axes = axes.ravel()

for ax, (cov_type, sample, sample_labels) in zip(axes, results):
    ax.scatter(sample[:, 0], sample[:, 1], sample[:, 2], c=sample_labels)
    ax.set_title(f"Covariance: {cov_type}")
    ax.set_xlabel("R")
    ax.set_ylabel("G")
    ax.set_zlabel("B")

plt.show()
```

**Тайлбар:** Зурган өгөгдлөөс санамсаргүйгээр авсан 1000 цэгийн 3 өөр кластерт хамаарах байдлыг өнгөөр харуулсан бөгөөд цэгүүдийн продорционал хамаарлыг зохих дүрслэлтэй нь (@fig-segmented-images) хамтатган дүгнэлт хийж болно. Үүнээс үргэлжлүүлэн дараагын хэсэгт 'tied' ковариацийн матрицийг ашигласан үед ямар учир дүрслэл илүү үр дүнтэй байгаа талаар дэлгэрэнгүй тайлбарлана.

Зургыг өөр хэсгүүдэд ялгахдаа ашиглах өнгүүдийг тодорхойлох.
```{python}
from matplotlib.colors import ListedColormap

segmented_images = []
colors = ["#000000", "#FFFFFF", "#707070"]
my_cmap = ListedColormap(colors, name="my3colors")
cov_types = ['full', 'tied', 'spherical', 'diag']

```

Өгөгдлийг сургаж дуусаад зурган хэлбэрт буцааж оруулна.
```{python}
#| label: fig-segmented-images
#| fig-cap: Ковариацийн матриц бүрийн төрөлд segment хийсэн зурган өгөгдлийн хувилбарууд.
#| fig-env: ''

for type in cov_types:
    gmm = GM(n_components=3, covariance_type=type).fit(img3)
    labels = gmm.predict(img3)

    colored = my_cmap(labels / labels.max())[:, :3]

    segmented_img = (colored * 255).astype(np.uint8).reshape(rows, cols, 3)
    final = Image.fromarray(segmented_img)
    segmented_images.append(final)


plt.style.use('default')
fig, axes = plt.subplots(2, 2, figsize=(10, 10))
axes = axes.ravel()

for ax, cov_type, seg_img in zip(axes, cov_types, segmented_images):
    ax.imshow(seg_img)
    ax.set_title(f"Covariance: {cov_type}")
    ax.axis('off')

plt.show()
```
\pagebreak

# ГХМ ковариацийн 'tied' үед яагаад зураг сэгмэнтлэл хамгийн үр өгөөжтэй байна вэ?

```{python}
from sklearn.metrics import silhouette_score

model_stats = []

for type in cov_types:
    gmm = GM(n_components=3, covariance_type=type, random_state=42).fit(img3)
    labels = gmm.predict(img3)

    colored = my_cmap(labels / labels.max())[:, :3]

    segmented_img = (colored * 255).astype(np.uint8).reshape(rows, cols, 3)
    final = Image.fromarray(segmented_img)
    segmented_images.append(final)
    
    # Статистик мэдээлэл цуглуулах
    n_params = gmm._n_parameters()
    bic = gmm.bic(img3)
    aic = gmm.aic(img3)
    log_likelihood = gmm.score(img3) * len(img3)
    converged = gmm.converged_
    n_iter = gmm.n_iter_
    
    sil_score = silhouette_score(img3[idx], labels[idx])
    
    model_stats.append({
        'Covariance': type,
        'Parameters': n_params,
        'Log-likelihood': f'{log_likelihood:.2e}',
        'BIC': f'{bic:.2e}',
        'AIC': f'{aic:.2e}',
        'Silhouette': f'{sil_score:.4f}',
        'Iteration': n_iter,
    })
```

```{python}
#| label: tbl-model-comparison
#| tbl-cap: ГХМ загваруудын параметр болон үнэлгээний үзүүлэлтүүд
import pandas as pd

df_stats = pd.DataFrame(model_stats)
df_stats
```


**Тайлбар:** BIC болон AIC үзүүлэлтүүд бага байх тусам сайн. Silhouette оноо өндөр байх тусам сайн (кластеруудын тодорхой байдлыг хэмждэг, -1-ээс 1 хүртэл). Log-likelihood өндөр байх тусам сайн (өгөгдөлд загвар тааруулалт сайн).

Яагаад ГХМ-д ковариансын матриксийн "tied" үед бусад төрлөөс илүү сегментацийн үр дүнтэй байна вэ?

ГХМ-ийн сегментацчилал нь загварт ашиглагдаж байгаа ковариансын матриксийн бүтцээс шууд хамааралтай. "Tied" ашиглахад ГХМ бүх кластеруудад нэг ижил матрикс хэрэглэдэг бөгөөд ингэж хязгаарласанаар бусад уян хатан ковариансын загваруудтай харьцуулахад чөлөөт параметрүүдийн тоог мэдэгдэхүйц бууруулж, дүрслэлийн төвөгтэй байдлыг хязгаарлаж, overfitting эрсдэлийг бууруулдаг.
Параметрийг бууруулснаар үндсэн кластерууд ижил геомерт шинж чанартай үед давуу тал болж өгдөг. Ийм нөхцөлд "tied" коварианс ашиглах нь өгөгдлийн тархалтыг төвөгтэй болголгүйгээр илэрхийлэх боломжтой бөгөөд үр дүнтэй загварчлалыг бий болгодог.
Өгөгдлийн хэмжээ бага эсвэл шуугиан ихтэй үед "tied" бүтэцийн коварианс тогтвортой тооцоо гаргадаг ба энэ нь илүү оновчтой кластеруудын загварыг бий болгодог.

# Зурган өгөгдөл хангалттгүй үр дүн үзүүлэх нөхцлүүд

Explanation here!

# What latent-variables? Why latent-variables? When latent-variables?

Explanation here!