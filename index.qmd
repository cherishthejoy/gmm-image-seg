---
jupyter: python3
title: "Gaussian Mixture Model Image Segmentation"
subtitle: ""
abstract: Энэхүү ажлаар өөр хэмжээст бүхий санамсаргүй утгууд дээр хэвийн тархалтыг ашиглан график гаргах, тэр дундаас гурван хэмжээст утга бүхий зурган өгөгдөл дээр Гауссын хольцын моделийг ашиглан өгөгдлийг сегментлэн, хангалттай болон хангалтгүй үр дүн үзүүлэх тохиолдлуудыг тодорхойлон дүгнэх болно. Мөн тухайн моделийн параметрүүдийг EM алгоритмын тусламжтайгаар хэрхэн үнэлэгдэж буй талаар дурдана.
author: "Б.Батзолбоо, Д.Мөнх-Оргил, Ц.Ням-Очир, Б.Өлзийнаран, Н.Тэргэл"
date: "2025-11-06"
date-format: "YYYY оны M-р сарын D"
toc: true
toc-depth: 3
toc-title: Агуулга
number-sections: true
format:
  pdf:
    fig-pos: "H"
    echo: true
    pdf-engine: xelatex
    papersize: a4paper
    geometry:
      - left=2cm
      - right=2cm
      - top=2cm
      - bottom=3cm
    include-in-header:
      - text: |
          \usepackage[english,mongolian]{babel}
          \usepackage{fontspec}
          % үндсэн текстийн шрифт
          \setmainfont{Times New Roman}
          % код хэсгийн шрифт
          \setmonofont{DejaVu Sans Mono}
          \AddToHook{env/Highlighting/begin}{\footnotesize}
          % үндсэн гарчиг
          \usepackage{titling}
          \pretitle{\begin{center}\LARGE\bfseries}
          \posttitle{\par\end{center}\vskip 1em}
          % сэдвийн зүйлчлэл хэсгийн шрифт
          \usepackage{titlesec}
          \titleformat{\section}{\normalfont\Large\bfseries\selectfont}{\thesection}{1em}{}
          \titleformat{\subsection}{\normalfont\large\bfseries\selectfont}{\thesubsection}{1em}{}
          \titleformat{\subsubsection}{\normalfont\normalsize\bfseries\selectfont}{\thesubsubsection}{1em}{}
          % сэдвийн жагсаалт доторх шрифт
          \usepackage{tocloft}
          \renewcommand{\cfttoctitlefont}{\Large\bfseries\fontspec{Times New Roman}}
          \renewcommand{\cftaftertoctitle}{\vskip 1em}
          \renewcommand{\cftsecfont}{\normalfont\selectfont}
          \renewcommand{\cftsecpagefont}{\normalfont\selectfont}
          \renewcommand{\cftsubsecfont}{\normalfont\selectfont}
          \renewcommand{\cftsubsecpagefont}{\normalfont\selectfont}
          \renewcommand{\contentsname}{Агуулга}
    latex-max-runs: 3
editor: source
execute:
  echo: false
crossref: 
  fig-title: Зураг
  fig-prefix: Зураг
  tbl-title: Хүснэгт
  tbl-prefix: Хүснэгт
fig-align: center
fig-env: "figure"
fig-height: 4
fig-width: 6
fig-pos: "!ht"
fig-format: pdf
fig-cap-location: top
tbl-cap-location: top
bibliography: references.bib
csl: ieee.csl
citeproc: true
link-citations: true
---

```{python}
#| echo: false

conclusion = list()
```

# Шаардлагатай багцууд {-}

Шаардлагатай багцуудыг дараах байдлаар урьдчилан суулгана.

`pip install numpy scikit-learn Pillow matplotlib`

# Өгөгдөл

Голлон ашиглах өгөгдөл нь зурган өгөгдөл байх бөгөөд "jpg", "jpeg", "png" гэсэн өргөтгөлтэй байна. Эндээс "png" өргөтгөлтэй зургууд нь ихэвчлэн альфа сувгийн мэдээллийг агуулдаг учир зохих аргыг ашиглана мэдээллийг зохих хэлбэрт оруулна. Зургуудыг хангалттай болон хангалтгүй үр дүнд хүрэх гэж хоёр ангилах бөгөөд, GMM ашиглан хангалттай буюу segmentation хийхэд тохиромжтой, зураг дээрх объектууд олон өнгөөс тогтсон, гэрлийн чанар муу гэх тодотголууд дээр суурилан хангалтгүй гэж тус тусад нь тодорхойлж болно. Нэг болон хоёр хэмжээст хэвийн тархалтуудын хувьд scikit-learn сангийн синтетик өгөгдөл үүсгэх аргыг ашиглах болно. Мөн энэ тайланд Гауссын хольцын моделийг "ГХМ" гэж товчилсон болно.

# Нэг хэмжээст ГХМ

Гауссын хольцын модель нь бусад кластерийн моделиудаас ялгарах онцлог нь цэгэн өгөгдөл бүр нь өөрийн тархалттай гэж үздэг.
```{python}
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from sklearn.mixture import GaussianMixture as GMM
from matplotlib.colors import LogNorm
import matplotlib.ticker as ticker
```

Синтетик аргаар хоёр өөр төрлийн хэвийн тархалт бүхий өгөгдлүүдийг үүсгээд, тооцоолол хийх боломжтой хэлбэрт оруулна.

$$
\mu_1, \mu_2 = \text{Математик дундаж}
$$
$$
\sigma^2_1, \sigma^2_2  = \text{Стандарт хазайлт}
$$
$$
\pi = \text{Хольцийн жин}
$$  

Синтетик өгөгдөл үүсгэхдээ дээрх параметрүүдийг тодорхойлон тархалтуудыг үүсгэнэ.
```{python}
rng = np.random.default_rng()
x = np.concatenate((rng.normal(1, 2, 1000), rng.normal(9, 3, 1000)))
f = x.reshape(-1, 1)
```

Гауссын хольцын модельд зохих ковариацийн матрицийн төрөл болон кластерийн тоог өгнө. Ковариацийн матрицын төрлүүдийг дараа дахин авч үзнэ.

```{python}
gmm = GMM(n_components=2, covariance_type='full')
gmm.fit(f)
x_axis = np.sort(x)
```

Постериор магадлалын томьёонд орлуулга хийн цэгэн утга бүрийн магадлалыг олно. Гамма нь тухайн ажиглагдсан цэгэн өгөгдөл нь аль кластерт хамаарах вэ гэдэг асуултад тодорхой хариулт өгдөг. Гамма болон latent-variable -ийн талаар төгсгөл хэсэгт дурдах болно.
$$
\gamma_i = \frac{\pi_1 \cdot \phi_2(x_i|\mu_2,\sigma^2_2)}{\pi_2 \cdot \phi_1(x_i|\mu_1,\sigma^2_1) + \pi_1 \cdot \phi_2(x_i|\mu_2,\sigma^2_2)}
$$

Бидний өгөгдөл 2 хэвийн тархалттай кластер учир:

$$
\gamma_j = 1 - \gamma_i
$$байна.

$$
\pi = \sum_{i = 1}^N(1-\gamma_i)/N
$$
$$
\mu_1 = \frac{\sum_{i = 1}^N(1-\gamma_i)x_i}{\sum_{i = 1}^N(1-\gamma_i)}
$$
$$
\mu_2 = \frac{\sum_{i = 1}^N\gamma_ix_i}{\sum_{i = 1}^N\gamma_i}
$$
$$
\sigma_1 =
\sqrt{
\frac{\sum_{i = 1}^N(1-\gamma_i)(x_i - \mu_1)^2}{\sum_{i = 1}^N(1-\gamma_i)}
}
$$
$$
\sigma_2 =
\sqrt{
\frac{\sum_{i = 1}^N\gamma_i(x_i - \mu_2)^2}{\sum_{i = 1}^N\gamma_i}
}$$

Сургасан моделиор тооцоолсон $\mu_1, \mu_2, \sigma_1, \sigma_2, \pi$ параметрүүдийн боломжит хамгийн их утгуудыг тооцоолон нягтын функцийн графикийг дүрсэлнэ.

```{python}
#| label: fig-1d-pdf
#| fig-cap: Синтетик өгөгдөлтэй 2 өөр хэвийн тархалтын нягтын функц

weights = gmm.weights_
means = gmm.means_
covars = gmm.covariances_

plt.hist(f, bins=100, histtype='bar', density=True, ec='white', alpha=1.0)
plt.plot(x_axis, weights[0] * stats.norm.pdf(x_axis, means[0], np.sqrt(covars[0])).ravel(), c = 'red')
plt.plot(x_axis, weights[1] * stats.norm.pdf(x_axis, means[1], np.sqrt(covars[1])).ravel(), c = 'green')

plt.grid()
plt.show()
```

\pagebreak

# Хоёр хэмжээст ГХМ

Хоёр хэмжээст хэвийн тархалтын санамсаргүй утгууд нь вектор байх ба өгөгдлийн хэмжээсийг дагаж параметрүүдийн хэмжээс ихэснэ.

Өгөгдлийг өмнөхтэй адил синтетикээр үүсгэх бөгөөд стандарт хазайлт нь ковариациин матриц хэлбэртэй болно. Ковариацийн матриц нь эллипсийн хоёр хэмжээст огторгуйн тэнхлэг болон хэмжээст шууд нөлөөлнө.

Доорх код нь гурван өөр төрлийн эллипс хэмжээстэй энгийн тархалт бүхий өгөгдлийг үүсгэнэ.

```{python}

n_samples = 300
rng = np.random.default_rng()
c1 = rng.standard_normal((n_samples, 2)) + np.array([20, 20])
dot_two = np.array([[0.0, -0.7], [3.5, 0.7]])
c2 = np.dot(rng.standard_normal((n_samples, 2)), dot_two)
dot_three = np.array([[0.0, 0.5], [0.5, 0.7]])
c3 = np.dot(rng.standard_normal((n_samples, 2)), dot_three) + np.array([-10, 30])
X_train = np.vstack([c1, c2, c3])

gmm = GMM(n_components=3, covariance_type="full").fit(X_train)
```
Гурван өөр төрлийн хэвийн тархалттай учир, кластерийн тоог '3' гэж өгнө.

ГХМ нь голлон EM алгоритмийг ашиглан моделийг сургадаг. Алгоритмын тодорхой үе шатад санамсаргүй утгуудын үржвэр нь өмнөх үржвэрийн утгаас төдийлөн ялгарахгүй бол параметрийн үнэлэлт нь эцэстээ хүрсэн гэж тооцдог.

$$
\mathcal{L}(\theta|x_i) = \prod_{i=1}^N f(x_i|\theta)
$$

Үнэний орыг тооцоолоход цэгэн утга болгоныг үржүүлэх шаардлага гардаг учир логарифм авснаар зөвхөн нийлбэрийг тооцоолохоор болгон хөнгөвчилдөг. Энэ нөхцөлд параметр үнэлэлт нь логарифм нийлбэрт өөрчлөлт гарахгүй болох хүртэл явагдана гэсэн үг. Ийнхүү логарифм үнэний орыг олох нь олон давуу талтай гэж тайлбарлаж болно.

$$
\ell(\theta|x_i) = \log\left(\prod_{i=1}^N f(x_i|\theta)\right) = \sum_{i=1}^N \log f(x_i|\theta)
$$
\pagebreak

Хоёр хэмжээст огторгуйд нягтын функцийг дүрслэх 2 хэмжээст огторгуйг үүсгэнэ.
```{python}
x = np.linspace(-40.0, 50.0, 200)
y = np.linspace(-40.0, 50.0, 200)
X, Y = np.meshgrid(x, y)
XX = np.array([X.ravel(), Y.ravel()]).T
Z = -gmm.score_samples(XX)
Z = Z.reshape(X.shape)
``` 


Хоёр хэмжээст огторгуйд нягтын функцийн шугаман загварыг дүрсэлнэ.
```{python}
#| label: fig-negative-log
#| fig-cap: Хоёр хэмжээст хэвийн тархалтын түвшний шугам
fmt = ticker.LogFormatterMathtext()
fmt.create_dummy_axis()

CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), 
                levels=np.logspace(0, 3, 10))

plt.clabel(CS, fmt=fmt, fontsize=5)

plt.scatter(X_train[:, 0], X_train[:, 1], 0.8, color='white', marker="+")
plt.gca().set_aspect('equal')
plt.show()
``` 
\pagebreak

# Гурван хэмжээст ГХМ болон Image Segmentation

Өмнө нь нэг болон хоёр хэмжээст ГХМ моделийг синтетик өгөгдөл дээр суурилан сургах талаар авч үзсэн. Нэг хэмжээст өгөгдөлд гэрлийн эрчим (intensity), хоёр хэмжээст өгөгдөлд [x, y] тэнхлэгт байх цэгэн утга тохиромжтой бол гурван хэмжээст өгөгдлийн хувьд зурган өгөгдлийн RGB @matt2024 суваг нь тохиромжтой гэж төсөөлж болно. 

$$
\begin{bmatrix}
X_1
\cr
X_2
\cr
\vdots
\cr
X_n
\end{bmatrix}
\sim
\mathcal{N}
\left(
\begin{bmatrix}
\mu_{X_1}
\cr
\mu_{X_2}
\cr
\vdots
\cr
\mu_{X_n}
\end{bmatrix},
\begin{bmatrix}
\sigma_{X_1}^2 & \sigma_{X_1, X_2} & \dots & \sigma_{X_1, X_n}
\cr
\sigma_{X_2, X_1} & \sigma_{X_2}^2 & & \vdots
\cr
\vdots & & \ddots
\cr
\sigma_{X_n, X_1} & \ldots & & \sigma_{X_n}^2
\end{bmatrix}
\right)
$$

X санамсаргүй векторыг $\mu$ дундаж утгын вектор болон $\Sigma$ ковариацийн матриц гэсэн параметрүүд бүхий олон хэмжээс хэвийн тархалттай гэдгийг $X \sim N(\mu, \Sigma)$ гэж тэмдэглэнэ @makhgal2025. Энэ зарчмыг баримтлан 3 хэмжээст хэвийн тархалт бүхий модельд зурган өгөгдлийг ашиглана гэвэл ийнхүү өөрчилж ашиглаж болно.

$$
\mu_i = {\mu_{R}, \mu_{G}, \mu_{B}}
$$

$$
\Sigma_i = 
\begin{bmatrix}
\sigma^2_{R} & \sigma_{RG} & \sigma_{RB}
\cr
\sigma_{RG} & \sigma^2_{G} & \sigma_{GB}
\cr
\sigma_{RB} & \sigma_{GB} & \sigma^2_{B}
\end{bmatrix}
$$

```{python}
import warnings
warnings.filterwarnings("ignore")
import numpy as np
from PIL import Image
from IPython.display import display
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture as GM
```

Ашиглагдах зурган өгөгдлийг оруулах.
```{python}
img = Image.open("images/good/fruits.jpg")
```

Зурган өгөгдөл нь альфа сувгийн мэдээллийг агуулдаг бол энгийн 'RGB' форматад оруулан хадгална.
```{python}
img = img.convert('RGB')
img2 = np.array(img)
rows, cols, ch = img2.shape
img3 = img2.reshape((-1, 3))
```

Өгөгдлөөс зөвхөн 1000 цэгийг санамсаргүйгээр сонгон авч ковариацийн матрицын 4 төрөлд тус тусад нь сургана.
```{python}
results = []
cov_types = ['full', 'tied', 'spherical', 'diag']
idx = np.random.choice(len(img3), size = 1000, replace=False)

for cov_type in cov_types:
    gmm = GM(n_components=3, covariance_type=cov_type).fit(img3)
    labels = gmm.predict(img3)

    sample = img3[idx]
    sample_labels = labels[idx]
    results.append((cov_type, sample, sample_labels))
```

Цэгэн өгөгдлүүдийг кластерт хамаарах байдлаар өөр өнгөөр дүрслэх.
```{python}
#| label: fig-random-data-points
#| fig-cap: Зурган өгөгдлөөс санамсаргүйгээр сонгосон цэгүүдийн тархалтын график
plt.style.use('default')
fig, axes = plt.subplots(2, 2, figsize=(12, 12), subplot_kw={'projection': '3d'})
axes = axes.ravel()

for ax, (cov_type, sample, sample_labels) in zip(axes, results):
    ax.scatter(sample[:, 0], sample[:, 1], sample[:, 2], c=sample_labels)
    ax.set_title(f"Covariance: {cov_type}")
    ax.set_xlabel("R")
    ax.set_ylabel("G")
    ax.set_zlabel("B")

plt.show()
```

**Тайлбар:** Зурган өгөгдлөөс санамсаргүйгээр авсан 1000 цэгийн 3 өөр кластерт хамаарах байдлыг өнгөөр харуулсан бөгөөд цэгүүдийн пропорционал хамаарлыг зохих дүрслэлтэй нь (@fig-segmented-images) хамтатган дүгнэлт хийж болно. Үүнээс үргэлжлүүлэн дараагийн хэсэгт 'tied' ковариацийн матрицыг ашигласан үед ямар учир дүрслэл илүү үр дүнтэй байгаа талаар дэлгэрэнгүй тайлбарлана.

\pagebreak

Зургийг өөр хэсгүүдэд ялгахдаа ашиглах өнгийг тодорхойлох.
```{python}
from matplotlib.colors import ListedColormap

segmented_images = []
colors = ["#000000", "#FFFFFF", "#707070"]
my_cmap = ListedColormap(colors, name="my3colors")
cov_types = ['full', 'tied', 'spherical', 'diag']

```

Өгөгдлийг сургаж дуусаад зурган хэлбэрт буцааж оруулна.
```{python}
#| label: fig-segmented-images
#| fig-cap: Ковариацийн матриц бүрийн төрөлд segment хийсэн зурган өгөгдлийн хувилбарууд.
#| fig-env: ''

for type in cov_types:
    gmm = GM(n_components=3, covariance_type=type).fit(img3)
    labels = gmm.predict(img3)

    colored = my_cmap(labels / labels.max())[:, :3]

    segmented_img = (colored * 255).astype(np.uint8).reshape(rows, cols, 3)
    final = Image.fromarray(segmented_img)
    segmented_images.append(final)


plt.style.use('default')
fig, axes = plt.subplots(2, 2, figsize=(10, 10))
axes = axes.ravel()

for ax, cov_type, seg_img in zip(axes, cov_types, segmented_images):
    ax.imshow(seg_img)
    ax.set_title(f"Covariance: {cov_type}")
    ax.axis('off')

plt.show()
```

**Тайлбар:** Ковариацийн матрицын 'tied' төрөлд зурагт буй объектуудын зааглал бусад төрлөөс илүү өндөр түвшинд харагдаж байгааг харч болно. 

\pagebreak

# ГХМ ковариацийн 'tied' төрөлд яагаад зураг сегментлэл хамгийн үр өгөөжтэй байна вэ?

```{python}
from sklearn.metrics import silhouette_score

model_stats = []

for type in cov_types:
    gmm = GM(n_components=3, covariance_type=type, random_state=42).fit(img3)
    labels = gmm.predict(img3)

    colored = my_cmap(labels / labels.max())[:, :3]

    segmented_img = (colored * 255).astype(np.uint8).reshape(rows, cols, 3)
    final = Image.fromarray(segmented_img)
    segmented_images.append(final)
    
    # Статистик мэдээлэл цуглуулах
    n_params = gmm._n_parameters()
    bic = gmm.bic(img3)
    aic = gmm.aic(img3)
    log_likelihood = gmm.score(img3) * len(img3)
    converged = gmm.converged_
    n_iter = gmm.n_iter_
    
    sil_score = silhouette_score(img3[idx], labels[idx])
    
    model_stats.append({
        'Covariance': type,
        'Parameters': n_params,
        'Log-likelihood': f'{log_likelihood:.2e}',
        'BIC': f'{bic:.2e}',
        'AIC': f'{aic:.2e}',
        'Silhouette': f'{sil_score:.4f}',
        'Iteration': n_iter,
    })
```

```{python}
#| label: tbl-model-comparison
#| tbl-cap: ГХМ загваруудын параметр болон үнэлгээний үзүүлэлтүүд
import pandas as pd

df_stats = pd.DataFrame(model_stats)
df_stats
```


**Тайлбар:** BIC болон AIC үзүүлэлтүүд бага байх тусам сайн. Silhouette оноо өндөр байх тусам сайн (кластеруудын тодорхой байдлыг хэмждэг, -1-ээс 1 хүртэл). Логарифм үнэний ор өндөр байх тусам сайн (өгөгдөлд загвар тааруулалт сайн).

Яагаад ГХМ-д ковариацийн матрицын "tied" үед бусад төрлөөс илүү сегментацийн үр дүнтэй байна вэ?

ГХМ-ийн сегментлэл нь загварт ашиглагдаж байгаа ковариансын матриксийн бүтцээс шууд хамааралтай. "Tied" ашиглахад ГХМ бүх кластеруудад нэг ижил матриц хэрэглэдэг бөгөөд ингэж хязгаарласанаар бусад уян хатан ковариансын загваруудтай харьцуулахад чөлөөт параметрүүдийн тоог мэдэгдэхүйц бууруулж, дүрслэлийн төвөгтэй байдлыг хязгаарлаж, overfitting эрсдэлийг бууруулдаг.
Параметрийг бууруулснаар үндсэн кластерууд ижил геомерт шинж чанартай үед давуу тал болж өгдөг. Ийм нөхцөлд "tied" ковариац матрицыг ашиглах нь өгөгдлийн тархалтыг төвөгтэй болголгүйгээр илэрхийлэх боломжтой бөгөөд үр дүнтэй загварчлалыг бий болгодог.
Өгөгдлийн хэмжээ бага эсвэл шуугиан ихтэй үед "tied" бүтцийн ковариац тогтвортой тооцоо гаргадаг ба энэ нь илүү оновчтой кластеруудын загварыг бий болгодог.

# Зурган өгөгдөл хангалтгүй үр дүн үзүүлэх нөхцөлүүд

Гауссын холимог загвар (ГХМ)-ын сегментлэл нь зураг дээрх пикселийн өнгө, тод байдлын утгуудыг хэд хэдэн Гауссын (хэвийн) тархалтад хуваарилж ажилладаг. Гэвч энэ загвар нь пикселийн орон зайн хамаарлыг тооцдоггүй, зөвхөн өнгөнд тулгуурладаг тул зарим төрлийн зургууд дээр маш муу ажилладаг. Хамгийн муу тохиолдлуудад дуу чимээ ихтэй зургууд багтана, учир нь ГХМ нь дуу чимээний пикселүүдийг бодит объектын нэг хэсэг мэтээр андуурч, сегментлэлийг тасархай болгодог. Объектуудын зааг бүдэг, өнгөний утгууд нь давхацсан зургууд бас хүндрэл учруулдаг, энэ үед загвар аль сегментэд хамаарахыг найдвартай шийдэж чаддаггүй. Эцэст нь, тэгш хэмт бус өгөгдлийн тархалттай зургууд ч мөн адил үр дүнг муутгадаг, учир нь ГХМ-ийн тэгш хэмт байдлын таамаглал нь бодит байдлаас зөрөхөд загвар хэт их төвөгтэй болж, буруу сегментлэл хүргэдэг.

```{python}
#| label: fig-worse-case
#| fig-cap: ГХМ өгөгдлийн хангалтгүй үр дүн үзүүлэх хувилбарууд
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from matplotlib.colors import ListedColormap

width, height = 100, 100
colors = ["#FF0000", "#00FF00", "#0000FF"]
cmap = ListedColormap(colors, name="three_colors")
# Worst-case 
pictures = []
# Low color difference 
low_colors_image = np.zeros((height, width, 3), dtype=np.uint8)
for i in range(height):
    for j in range(width):
        r = 50 + (i + j) % 30     
        g = 60 + (i * j) % 30     
        b = 55 + (i + 2*j) % 30   
        low_colors_image[i,j] = [r,g,b]

pictures.append(("Low Color Difference (slightly)", low_colors_image))
#  High noise
noise_image = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)
pictures.append(("High Noise", noise_image))
#  Many small objects (checkerboard)
checker_image = np.zeros((height, width, 3), dtype=np.uint8)
for i in range(height):
    for j in range(width):
        if (i//5 + j//5) % 2 == 0:
            checker_image[i,j] = [255,0,0]
        else:
            checker_image[i,j] = [0,0,255]
pictures.append(("Many Small Objects", checker_image))
# 3. Apply GMM segmentation
for title, image in pictures:
    pixels = image.reshape(-1, 3)
    gmm = GaussianMixture(n_components=3, covariance_type='tied', random_state=42)
    gmm.fit(pixels)
    labels = gmm.predict(pixels)
    
    segmented = cmap(labels / labels.max())[:, :3]
    segmented_image = (segmented * 255).astype(np.uint8).reshape(height, width, 3)
    
    plt.figure(figsize=(6,5))
    plt.subplot(1,2,1)
    plt.imshow(image)
    plt.title(f"Original\n{title}", fontsize=10)
    plt.axis('off')
    
    plt.subplot(1,2,2)
    plt.imshow(segmented_image)
    plt.title("GMM Segmentation", fontsize=10)
    plt.axis('off')
    
    plt.show()
```

# Үл мэдэгдэх хувьсагч

"Latent variable" буюу үл мэдэгдэх хувьсагч нь зарим статистикийн моделиудад дурдагддаг ойлголт ба энэхүү утга нь тооцоолол эсвэл дүгнэлтэд нөлөөлдөг (нөлөөлдөггүй) @kamp2021 боловч нүдэнд үзэгддэггүй, шууд тайлбарлагддаггүй утга юм. 

$$
\pi = \sum_{i = 1}^N(1-\gamma_i)/N
$$

Бидний авч үзсэн ГХМ -д EM алгоритмын үе шатад параметр тус бүрийг "гамма" утгын тусламжтайгаар дахин үнэлж байсныг эргэн харч болно. Энэ тооцоолсон гамма утга нь цэг тус бүр тэрхүү кластерт ямар магадлалтай хамаарах вэ гэдэг асуултад сургасан моделийн тусламжтайгаар постериор магадлалаар хариу өгч байна гэж ойлгож болно.



# Дүгнэлт {.unnumbered}

1. Зураг сегментлэлийн үр дүн нь зурган өгөгдлийн шуугиан, объектын тоо, өнгөний ялгарлаас шууд хамаарч байгаа бөгөөд хэт их шуугиан, объектийн тоо болон хэт бага өнгөний ялгарал нь үр дүнгүй сэгмэнтчлэлд хүргэж байна.
2. Үр дүн нь ковариацийн матрицтай шууд хамааралтай бөгөөд 'tied' үед үр дүн хамгийн өндөр байна.
3. ГХМ ерөнхий тохиолдолд өгөгдлийг хэвийн буюу гауссын тархалттай гэж үзэн тооцоолол хийдэг бол өгөгдлийг машин сургалт эсвэл зурган өгөгдөл засварлах арга техникийг ашиглан өгөгдлийн нийцтэй байдлыг хангах боломжтой гэж харч байна. Жишээлбэл: "Quantization" буюу тасралтгүй цэгэн утгуудыг тоологдохуйц дискрет утгуудад хөрвүүлэх арга байж болно.
  

# Ашигласан материал {.unnumbered}

::: {#refs}
:::