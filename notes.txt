You have a n-dim of vectors or just an array of scalar values.
You distribute it along the axis, and you notice that there are 2 different bell curves.
But Gaussian/Normal distributions have only 1 mean curve.

You think, maybe this is a problem for Gaussian Mixture Model?

The whole point of the model is to find whether the observation you have seen 
belongs to either one or the other clusters/classes.

Each cluster corresponds to a Gaussian distribution.

It moves hard clustering -> soft clustering, where you think each data point
can belong to each cluster with certain probability.

Cluster shapes:

- Mean 
    The mean represents the central point in the feature space.
- Covariance
    The covariance matrix describes the shape, size and orientation
    of the cluster or ellipse in some cases. Unlike simpler clustering methods
    such as K-means which assume spherical clusters. The shape of the elliptical
    cluster is directly related to the covariance matrix where the relationships 
    between features define tilt, stretch and compression.



Steps of calculation:

- Expectation Maximization

In statistics, EM algorithm iteratively aims to find the local maximum likelhood 
or maximum a posteriori of parameters in statistical models in cases where 
the equation cannot be solved directly. 
As you can already tell, the algorithm uses Maximum Likelihood Estimation (MLE) 
to find the best possible values for given parameters.

In EM algorithm we try to find the best values for 
Mu (Mean), 
Sigma (Covariance), 
Pi (Mixing coefficient/Mixture weights)

Initialization:

Start with initial guesses for the Theta(Parameters) for each Gaussian distributions.

Expectation step:
    Calculate gamma (responsibility)
    For each data point calculate the responsibility/probability of it belonging to each
    Gaussian distribution (cluster)
    Assign soft responsibility


Maximization step:

    Update the parameters based on gamma



Other information:

    The Covariance Matrix:
        [Var(x) Cov(x, y)]
        [Cov(x, y) Var(y)]

- Reason we take log-likelhood instead of likelihood
When we try to find total likelihood, we calculate the product of each data point.
Taking log likelihood here would simplify the product to sum. log(a * b) = log(a) + log(b).
Also, after multiplying thousands of probabilities of points, we have risk of underflowing of tiny numbers.
Log of numbers between 0 and 1 are negative numbers.
So, taking the log of the likelihood is a convenience.

- The hidden variable problem:

    There's this hidden variable (z) exists which has an intuition telling which data point belongs
    to either one of the clusters. Instead pondering about this hidden value, we try to make sense of it 
    with gamma (the responsibility).
    In another words, gamma is our probabilistic guess of this hidden variable.
        






